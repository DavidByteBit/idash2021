{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa9454b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dc33305",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "\n",
    "def preprocess(dirty_df):\n",
    "  dirty_df = dirty_df.drop(['patient_id','cohort_type'], axis = 1)\n",
    "  target_map = {u'1': 1, u'0': 0}\n",
    "  dirty_df['__target__'] = dirty_df['cohort_flag'].map(str).map(target_map)\n",
    "  dirty_df = dirty_df.drop(['cohort_flag'], axis = 1)\n",
    "  clean_X = dirty_df.drop('__target__', axis=1)\n",
    "  clean_y = np.array(dirty_df['__target__'])\n",
    "  return clean_X, clean_y\n",
    "\n",
    "def preprocess_normalizationx(dirty_df):\n",
    "  dirty_df = dirty_df.drop(['patient_id','cohort_type'], axis = 1)\n",
    "  target_map = {u'1': 1, u'0': 0}\n",
    "  dirty_df['__target__'] = dirty_df['cohort_flag'].map(str).map(target_map)\n",
    "  dirty_df = dirty_df.drop(['cohort_flag'], axis = 1)\n",
    "  clean_X = dirty_df.drop('__target__', axis=1)\n",
    "  clean_X = clean_X.to_numpy()\n",
    "  clean_X = preprocessing.normalize(clean_X, norm='l2')\n",
    "  clean_y = np.array(dirty_df['__target__'])\n",
    "  return clean_X, clean_y\n",
    "\n",
    "\n",
    "def evaluateForest(ntrees, RFresults, X_train, y_train, X1_train, y1_train, X2_train, y2_train, X_test, y_test): \n",
    "  clf1 = RandomForestClassifier(n_estimators=ntrees,random_state=10000)\n",
    "  clf2 = RandomForestClassifier(n_estimators=ntrees,random_state=10000)\n",
    "  clf  = RandomForestClassifier(n_estimators=ntrees,random_state=10000)\n",
    "  \n",
    "  clf1.fit(X1_train, y1_train)\n",
    "  accP1  = accuracy_score(y_test,clf1.predict(X_test))\n",
    "\n",
    "  clf2.fit(X2_train, y2_train)\n",
    "  accP2 = accuracy_score(y_test,clf2.predict(X_test))\n",
    "    \n",
    "  clf.fit(X_train, y_train)\n",
    "  accALL = accuracy_score(y_test,clf.predict(X_test))\n",
    "  \n",
    "  # Merging of RF models  \n",
    "  clf1.estimators_ += clf2.estimators_\n",
    "  clf1.n_estimators = len(clf1.estimators_)\n",
    "  accMERG = accuracy_score(y_test,clf1.predict(X_test))\n",
    "\n",
    "  RFresults[i] = [accP1,accP2,accALL,accMERG]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bc701ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "\n",
    "# Load the data\n",
    "df1 = pd.read_csv('party_1.csv')\n",
    "df2 = pd.read_csv('party_2.csv')\n",
    "\n",
    "X1, y1 = preprocess_normalizationx(df1)\n",
    "X2, y2 = preprocess_normalizationx(df2)\n",
    "\n",
    "n_train=700\n",
    "n_test=131\n",
    "train_y1=y1[0:n_train]\n",
    "train_X1=X1[0:n_train]\n",
    "test_y1=y1[n_train:n_train+n_test]\n",
    "test_x1=X1[n_train:n_train+n_test]    \n",
    "    \n",
    "train_y1=pd.DataFrame(train_y1)\n",
    "train_X1=pd.DataFrame(train_X1)\n",
    "test_y1=pd.DataFrame(test_y1)\n",
    "test_x1=pd.DataFrame(test_x1)\n",
    "\n",
    "n_times = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ddebc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_mpc(list_files, out_file_mpc, n_train, n_test):\n",
    "    # part    \n",
    "    train_y1=y1[0:n_train]\n",
    "    train_X1=X1[0:n_train]\n",
    "    test_y1=y1[n_train:n_train+n_test]\n",
    "    test_x1=X1[n_train:n_train+n_test]    \n",
    "\n",
    "    train_y1=pd.DataFrame(train_y1)\n",
    "    train_X1=pd.DataFrame(train_X1)\n",
    "    test_y1=pd.DataFrame(test_y1)\n",
    "    test_x1=pd.DataFrame(test_x1)\n",
    "    \n",
    "    train_X1.to_csv(list_files[0], header=False, sep=' ', index=False)\n",
    "    y1_aux = pd.DataFrame(train_y1)\n",
    "    y1_aux.to_csv(list_files[1], header=False, sep=' ', index=False)\n",
    "    test_x1.to_csv(list_files[2], header=False, sep=' ', index=False)\n",
    "    y2_aux = pd.DataFrame(test_y1)\n",
    "    y2_aux.to_csv(list_files[3], header=False, sep=' ', index=False)\n",
    "    \n",
    "    \n",
    "    filenames = list_files\n",
    "    with open(out_file_mpc, 'w') as outfile:\n",
    "        for fname in filenames:\n",
    "            with open(fname) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "                    \n",
    "generate_input_mpc(['Player-Data/train_X1', 'Player-Data/train_y1', 'Player-Data/test_x1', 'Player-Data/test_y1'], 'Player-Data/Input-P0-0', n_train=700, n_test=131)\n",
    "\n",
    "generate_input_mpc(['Player-Data/train_X2', 'Player-Data/train_y2', 'Player-Data/test_x2', 'Player-Data/test_y2'], 'Player-Data/Input-P1-0', n_train=700, n_test=131)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e66772d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(831, 1877)\n",
      "Starting to process fold 5\n",
      "Starting to process fold 6\n"
     ]
    }
   ],
   "source": [
    "# date: July 22, 2021\n",
    "# name: Martine De Cock\n",
    "# description: Training ML models on IDASH2021, Track 3 data\n",
    "\n",
    "# DP LR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from numpy import savetxt\n",
    "import sys\n",
    "\n",
    "rows = None\n",
    "cols = None\n",
    "num_of_folds = 2\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "def preprocess(dirty_df):\n",
    "    dirty_df = dirty_df.drop(['patient_id', 'cohort_type'], axis=1)\n",
    "    target_map = {u'1': 1, u'0': 0}\n",
    "    dirty_df['__target__'] = dirty_df['cohort_flag'].map(str).map(target_map)\n",
    "    dirty_df = dirty_df.drop(['cohort_flag'], axis=1)\n",
    "    clean_X = dirty_df.drop('__target__', axis=1)\n",
    "\n",
    "    if cols is not None:\n",
    "        clean_X = clean_X.iloc[:, :cols]\n",
    "\n",
    "    clean_X = clean_X.to_numpy()\n",
    "    clean_X = preprocessing.normalize(clean_X, norm='l2')\n",
    "    clean_y = np.array(dirty_df['__target__'])\n",
    "\n",
    "    if rows is not None:\n",
    "        return clean_X[0:rows], clean_y[0:rows]\n",
    "\n",
    "    return clean_X, clean_y\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df1 = pd.read_csv('data/alice_data.csv')\n",
    "df2 = pd.read_csv('data/bob_data.csv')\n",
    "\n",
    "print(df1.shape)\n",
    "\n",
    "X1, y1 = preprocess(df1)\n",
    "X2, y2 = preprocess(df2)\n",
    "\n",
    "# This will hold 4 accuracy results for each of 5 folds\n",
    "# (1) accuracy of model trained on data from P1\n",
    "# (2) accuracy of model trained on data from P2\n",
    "# (3) accuracy of model trained on data from P1 and from P2\n",
    "# (4) accuracy of aggregation of model (1) and (2) from above\n",
    "LRresults = np.zeros((5, 5))\n",
    "\n",
    "kf1 = KFold(n_splits=num_of_folds, shuffle=True, random_state=42)\n",
    "kf2 = KFold(n_splits=num_of_folds, shuffle=True, random_state=42)\n",
    "\n",
    "epsilon = 1\n",
    "mylambda = 0.5\n",
    "\n",
    "fold = 5\n",
    "\n",
    "for (train1_indices, test1_indices), (train2_indices, test2_indices) in zip(kf1.split(X1, y1), kf2.split(X2, y2)):\n",
    "\n",
    "    print(\"Starting to process fold {n}\".format(n=fold))\n",
    "\n",
    "    AliceX_train, AliceX_test = X1[train1_indices, :].tolist(), X1[test1_indices, :].tolist()\n",
    "    Alicey_train, Alicey_test = y1[train1_indices].tolist(), y1[test1_indices].tolist()\n",
    "    BobX_train, BobX_test = X2[train2_indices, :].tolist(), X2[test2_indices, :].tolist()\n",
    "    Boby_train, Boby_test = y2[train2_indices].tolist(), y2[test2_indices].tolist()\n",
    "\n",
    "    # Get rid of scientific notation\n",
    "    AliceX_train = [[str(f'{j:.10f}') for j in i] for i in AliceX_train]\n",
    "    AliceX_test = [[str(f'{j:.10f}') for j in i] for i in AliceX_test]\n",
    "    Alicey_train = [str(f'{i:.10f}') for i in Alicey_train]\n",
    "    Alicey_test = [str(f'{i:.10f}') for i in Alicey_test]\n",
    "\n",
    "    BobX_train = [[str(f'{j:.10f}') for j in i] for i in BobX_train]\n",
    "    BobX_test = [[str(f'{j:.10f}') for j in i] for i in BobX_test]\n",
    "    Boby_train = [str(f'{i:.10f}') for i in Boby_train]\n",
    "    Boby_test = [str(f'{i:.10f}') for i in Boby_test]\n",
    "\n",
    "    savetxt('data/Alice/train_X_fold{n}.csv'.format(n=fold), AliceX_train, delimiter=',', fmt='%s')\n",
    "    savetxt('data/Alice/train_y_fold{n}.csv'.format(n=fold), Alicey_train, delimiter=',', fmt='%s')\n",
    "    savetxt('data/Alice/test_X_fold{n}.csv'.format(n=fold), AliceX_test, delimiter=',', fmt='%s')\n",
    "    savetxt('data/Alice/test_y_fold{n}.csv'.format(n=fold), Alicey_test, delimiter=',', fmt='%s')\n",
    "\n",
    "    savetxt('data/Bob/train_X_fold{n}.csv'.format(n=fold), BobX_train, delimiter=',', fmt='%s')\n",
    "    savetxt('data/Bob/train_y_fold{n}.csv'.format(n=fold), Boby_train, delimiter=',', fmt='%s')\n",
    "    savetxt('data/Bob/test_X_fold{n}.csv'.format(n=fold), BobX_test, delimiter=',', fmt='%s')\n",
    "    savetxt('data/Bob/test_y_fold{n}.csv'.format(n=fold), Boby_test, delimiter=',', fmt='%s')\n",
    "\n",
    "    fold += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd3f74d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_mpc1(list_files, out_file_mpc):\n",
    "    filenames = list_files\n",
    "    with open(out_file_mpc, 'w') as outfile:\n",
    "        for fname in filenames:\n",
    "            with open(fname) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "                    \n",
    "generate_input_mpc1(['data/Alice/train_X_fold0.csv', 'data/Alice/train_y_fold0.csv', 'data/Alice/test_X_fold0.csv', 'data/Alice/test_y_fold0.csv'], 'data/Input-P0-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9357ac8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix\n",
      " [[58 13]\n",
      " [ 4 91]]\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.82      0.87        71\n",
      "         1.0       0.88      0.96      0.91        95\n",
      "\n",
      "    accuracy                           0.90       166\n",
      "   macro avg       0.91      0.89      0.89       166\n",
      "weighted avg       0.90      0.90      0.90       166\n",
      "\n",
      "confusion_matrix\n",
      " [[58 13]\n",
      " [ 4 91]]\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.82      0.87        71\n",
      "         1.0       0.88      0.96      0.91        95\n",
      "\n",
      "    accuracy                           0.90       166\n",
      "   macro avg       0.91      0.89      0.89       166\n",
      "weighted avg       0.90      0.90      0.90       166\n",
      "\n",
      "confusion_matrix\n",
      " [[58 13]\n",
      " [ 4 91]]\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.82      0.87        71\n",
      "         1.0       0.88      0.96      0.91        95\n",
      "\n",
      "    accuracy                           0.90       166\n",
      "   macro avg       0.91      0.89      0.89       166\n",
      "weighted avg       0.90      0.90      0.90       166\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "train_X1 = pd.read_csv('data/Alice/train_X_fold0.csv')\n",
    "train_y1 = pd.read_csv('data/Alice/train_y_fold0.csv')\n",
    "test_x1 = pd.read_csv('data/Alice/test_X_fold0.csv')\n",
    "test_y1 = pd.read_csv('data/Alice/test_y_fold0.csv')\n",
    "\n",
    "for i in range(n_times):\n",
    "    perceptron_sklearn = Perceptron(max_iter=200)\n",
    "    perceptron_sklearn.fit(train_X1,train_y1)\n",
    "\n",
    "    resultado_predicao = perceptron_sklearn.predict(test_x1)\n",
    "\n",
    "    print('confusion_matrix\\n', confusion_matrix(resultado_predicao, test_y1))\n",
    "    print('classification_report\\n',classification_report(resultado_predicao, test_y1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71b0e89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix\n",
      " [[33  6]\n",
      " [10 82]]\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.80        39\n",
      "           1       0.93      0.89      0.91        92\n",
      "\n",
      "    accuracy                           0.88       131\n",
      "   macro avg       0.85      0.87      0.86       131\n",
      "weighted avg       0.88      0.88      0.88       131\n",
      "\n",
      "confusion_matrix\n",
      " [[33  6]\n",
      " [10 82]]\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.80        39\n",
      "           1       0.93      0.89      0.91        92\n",
      "\n",
      "    accuracy                           0.88       131\n",
      "   macro avg       0.85      0.87      0.86       131\n",
      "weighted avg       0.88      0.88      0.88       131\n",
      "\n",
      "confusion_matrix\n",
      " [[33  6]\n",
      " [10 82]]\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.80        39\n",
      "           1       0.93      0.89      0.91        92\n",
      "\n",
      "    accuracy                           0.88       131\n",
      "   macro avg       0.85      0.87      0.86       131\n",
      "weighted avg       0.88      0.88      0.88       131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for i in range(n_times):\n",
    "    regr = LogisticRegression(max_iter=200)\n",
    "    regr.fit(train_X1,train_y1)\n",
    "    resultado_predicao = regr.predict(test_x1)\n",
    "    print('confusion_matrix\\n', confusion_matrix(resultado_predicao, test_y1))\n",
    "    print('classification_report\\n',classification_report(resultado_predicao, test_y1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "534028fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 1)                 1875      \n",
      "=================================================================\n",
      "Total params: 1,875\n",
      "Trainable params: 1,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "22/22 [==============================] - 1s 2ms/step - loss: 0.6878 - accuracy: 0.5871\n",
      "Epoch 2/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6858 - accuracy: 0.5914\n",
      "Epoch 3/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6840 - accuracy: 0.5914\n",
      "Epoch 4/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6824 - accuracy: 0.5900\n",
      "Epoch 5/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6810 - accuracy: 0.5886\n",
      "Epoch 6/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6797 - accuracy: 0.5886\n",
      "Epoch 7/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6785 - accuracy: 0.5886\n",
      "Epoch 8/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6774 - accuracy: 0.5886\n",
      "Epoch 9/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6765 - accuracy: 0.5886\n",
      "Epoch 10/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6756 - accuracy: 0.5886\n",
      "Epoch 11/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6748 - accuracy: 0.5886\n",
      "Epoch 12/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6741 - accuracy: 0.5886\n",
      "Epoch 13/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6734 - accuracy: 0.5886\n",
      "Epoch 14/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6727 - accuracy: 0.5886\n",
      "Epoch 15/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6721 - accuracy: 0.5886\n",
      "Epoch 16/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6716 - accuracy: 0.5886\n",
      "Epoch 17/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6710 - accuracy: 0.5886\n",
      "Epoch 18/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6705 - accuracy: 0.5886\n",
      "Epoch 19/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6700 - accuracy: 0.5886\n",
      "Epoch 20/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6695 - accuracy: 0.5886\n",
      "Epoch 21/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6691 - accuracy: 0.5886\n",
      "Epoch 22/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6686 - accuracy: 0.5886\n",
      "Epoch 23/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6682 - accuracy: 0.5886\n",
      "Epoch 24/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6678 - accuracy: 0.5886\n",
      "Epoch 25/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6674 - accuracy: 0.5886\n",
      "Epoch 26/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6670 - accuracy: 0.5886\n",
      "Epoch 27/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6667 - accuracy: 0.5886\n",
      "Epoch 28/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.6663 - accuracy: 0.5886\n",
      "Epoch 29/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6659 - accuracy: 0.5886\n",
      "Epoch 30/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6656 - accuracy: 0.5886\n",
      "Epoch 31/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6652 - accuracy: 0.5886: 0s - loss: 0.6627 - accuracy: 0.59\n",
      "Epoch 32/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6649 - accuracy: 0.5886\n",
      "Epoch 33/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6645 - accuracy: 0.5886\n",
      "Epoch 34/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6642 - accuracy: 0.5886\n",
      "Epoch 35/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.6638 - accuracy: 0.5886\n",
      "Epoch 36/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6635 - accuracy: 0.5886\n",
      "Epoch 37/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6632 - accuracy: 0.5886\n",
      "Epoch 38/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6628 - accuracy: 0.5886\n",
      "Epoch 39/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6625 - accuracy: 0.5886\n",
      "Epoch 40/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6621 - accuracy: 0.5886\n",
      "Epoch 41/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6618 - accuracy: 0.5886\n",
      "Epoch 42/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.5886\n",
      "Epoch 43/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.6612 - accuracy: 0.5886\n",
      "Epoch 44/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6608 - accuracy: 0.5886\n",
      "Epoch 45/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.5886\n",
      "Epoch 46/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6602 - accuracy: 0.5886\n",
      "Epoch 47/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6599 - accuracy: 0.5886\n",
      "Epoch 48/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6595 - accuracy: 0.5886\n",
      "Epoch 49/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6592 - accuracy: 0.5886\n",
      "Epoch 50/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6589 - accuracy: 0.5886\n",
      "Epoch 51/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6586 - accuracy: 0.5886\n",
      "Epoch 52/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6583 - accuracy: 0.5886\n",
      "Epoch 53/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6579 - accuracy: 0.5886\n",
      "Epoch 54/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6576 - accuracy: 0.5886\n",
      "Epoch 55/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.5886\n",
      "Epoch 56/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6570 - accuracy: 0.5886\n",
      "Epoch 57/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6567 - accuracy: 0.5886\n",
      "Epoch 58/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6564 - accuracy: 0.5886\n",
      "Epoch 59/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6561 - accuracy: 0.5886\n",
      "Epoch 60/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6557 - accuracy: 0.5886\n",
      "Epoch 61/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6554 - accuracy: 0.5886\n",
      "Epoch 62/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6551 - accuracy: 0.5886\n",
      "Epoch 63/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6548 - accuracy: 0.5886\n",
      "Epoch 64/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6545 - accuracy: 0.5886\n",
      "Epoch 65/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6542 - accuracy: 0.5886\n",
      "Epoch 66/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6538 - accuracy: 0.5886\n",
      "Epoch 67/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6535 - accuracy: 0.5886\n",
      "Epoch 68/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6533 - accuracy: 0.5886\n",
      "Epoch 69/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.6529 - accuracy: 0.5886\n",
      "Epoch 70/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6526 - accuracy: 0.5886\n",
      "Epoch 71/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6523 - accuracy: 0.5886\n",
      "Epoch 72/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6520 - accuracy: 0.5886\n",
      "Epoch 73/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6517 - accuracy: 0.5886\n",
      "Epoch 74/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6514 - accuracy: 0.5886\n",
      "Epoch 75/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6510 - accuracy: 0.5886\n",
      "Epoch 76/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6507 - accuracy: 0.5886\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6504 - accuracy: 0.5886\n",
      "Epoch 78/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6501 - accuracy: 0.5886\n",
      "Epoch 79/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6498 - accuracy: 0.5886\n",
      "Epoch 80/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6495 - accuracy: 0.5886\n",
      "Epoch 81/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6492 - accuracy: 0.5886\n",
      "Epoch 82/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6489 - accuracy: 0.5886\n",
      "Epoch 83/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6486 - accuracy: 0.5886\n",
      "Epoch 84/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6483 - accuracy: 0.5886\n",
      "Epoch 85/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6480 - accuracy: 0.5886\n",
      "Epoch 86/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6476 - accuracy: 0.5886\n",
      "Epoch 87/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6474 - accuracy: 0.5886\n",
      "Epoch 88/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6470 - accuracy: 0.5886\n",
      "Epoch 89/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6467 - accuracy: 0.5886\n",
      "Epoch 90/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6464 - accuracy: 0.5886\n",
      "Epoch 91/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6461 - accuracy: 0.5886\n",
      "Epoch 92/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6458 - accuracy: 0.5886\n",
      "Epoch 93/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6455 - accuracy: 0.5886\n",
      "Epoch 94/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6452 - accuracy: 0.5886\n",
      "Epoch 95/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6449 - accuracy: 0.5886\n",
      "Epoch 96/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6446 - accuracy: 0.5886\n",
      "Epoch 97/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6443 - accuracy: 0.5886\n",
      "Epoch 98/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6440 - accuracy: 0.5886\n",
      "Epoch 99/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.5886\n",
      "Epoch 100/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6434 - accuracy: 0.5886\n",
      "Epoch 101/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6431 - accuracy: 0.5886\n",
      "Epoch 102/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6428 - accuracy: 0.5886\n",
      "Epoch 103/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6425 - accuracy: 0.5886\n",
      "Epoch 104/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6422 - accuracy: 0.5886\n",
      "Epoch 105/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6419 - accuracy: 0.5886\n",
      "Epoch 106/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.6416 - accuracy: 0.5886\n",
      "Epoch 107/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6413 - accuracy: 0.5886\n",
      "Epoch 108/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6410 - accuracy: 0.5886\n",
      "Epoch 109/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6407 - accuracy: 0.5886\n",
      "Epoch 110/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6404 - accuracy: 0.5886\n",
      "Epoch 111/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6401 - accuracy: 0.5886\n",
      "Epoch 112/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6398 - accuracy: 0.5886\n",
      "Epoch 113/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6395 - accuracy: 0.5886\n",
      "Epoch 114/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6392 - accuracy: 0.5886\n",
      "Epoch 115/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6389 - accuracy: 0.5886\n",
      "Epoch 116/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6386 - accuracy: 0.5886\n",
      "Epoch 117/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6383 - accuracy: 0.5886\n",
      "Epoch 118/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6380 - accuracy: 0.5886\n",
      "Epoch 119/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6377 - accuracy: 0.5886\n",
      "Epoch 120/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.5886\n",
      "Epoch 121/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6372 - accuracy: 0.5886\n",
      "Epoch 122/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6369 - accuracy: 0.5886\n",
      "Epoch 123/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6366 - accuracy: 0.5886\n",
      "Epoch 124/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6363 - accuracy: 0.5886\n",
      "Epoch 125/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6360 - accuracy: 0.5886\n",
      "Epoch 126/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6357 - accuracy: 0.5886\n",
      "Epoch 127/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6354 - accuracy: 0.5886\n",
      "Epoch 128/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6351 - accuracy: 0.5886\n",
      "Epoch 129/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6348 - accuracy: 0.5886\n",
      "Epoch 130/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6346 - accuracy: 0.5886\n",
      "Epoch 131/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6342 - accuracy: 0.5886\n",
      "Epoch 132/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6339 - accuracy: 0.5886\n",
      "Epoch 133/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6337 - accuracy: 0.5886\n",
      "Epoch 134/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6334 - accuracy: 0.5886\n",
      "Epoch 135/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6331 - accuracy: 0.5886\n",
      "Epoch 136/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6328 - accuracy: 0.5886\n",
      "Epoch 137/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6325 - accuracy: 0.5886\n",
      "Epoch 138/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6322 - accuracy: 0.5886\n",
      "Epoch 139/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6319 - accuracy: 0.5886\n",
      "Epoch 140/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6316 - accuracy: 0.5886\n",
      "Epoch 141/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.6314 - accuracy: 0.5886\n",
      "Epoch 142/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6310 - accuracy: 0.5886\n",
      "Epoch 143/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6308 - accuracy: 0.5886\n",
      "Epoch 144/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6305 - accuracy: 0.5886\n",
      "Epoch 145/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6302 - accuracy: 0.5886\n",
      "Epoch 146/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6299 - accuracy: 0.5886\n",
      "Epoch 147/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6296 - accuracy: 0.5886\n",
      "Epoch 148/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6293 - accuracy: 0.5886\n",
      "Epoch 149/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6290 - accuracy: 0.5886\n",
      "Epoch 150/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6288 - accuracy: 0.5886\n",
      "Epoch 151/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6285 - accuracy: 0.5886\n",
      "Epoch 152/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6282 - accuracy: 0.5886\n",
      "Epoch 153/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6279 - accuracy: 0.5886\n",
      "Epoch 154/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6276 - accuracy: 0.5886\n",
      "Epoch 155/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6273 - accuracy: 0.5886\n",
      "Epoch 156/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6271 - accuracy: 0.5886\n",
      "Epoch 157/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6268 - accuracy: 0.5886\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6265 - accuracy: 0.5886\n",
      "Epoch 159/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6262 - accuracy: 0.5886\n",
      "Epoch 160/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6259 - accuracy: 0.5886\n",
      "Epoch 161/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6256 - accuracy: 0.5886\n",
      "Epoch 162/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6254 - accuracy: 0.5886\n",
      "Epoch 163/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6251 - accuracy: 0.5886\n",
      "Epoch 164/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6248 - accuracy: 0.5886\n",
      "Epoch 165/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6245 - accuracy: 0.5886\n",
      "Epoch 166/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6243 - accuracy: 0.5886\n",
      "Epoch 167/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6240 - accuracy: 0.5886\n",
      "Epoch 168/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6237 - accuracy: 0.5886\n",
      "Epoch 169/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6234 - accuracy: 0.5886\n",
      "Epoch 170/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6231 - accuracy: 0.5886\n",
      "Epoch 171/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6229 - accuracy: 0.5900\n",
      "Epoch 172/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6226 - accuracy: 0.5886\n",
      "Epoch 173/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6223 - accuracy: 0.5900\n",
      "Epoch 174/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6220 - accuracy: 0.5900\n",
      "Epoch 175/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6217 - accuracy: 0.5900\n",
      "Epoch 176/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6215 - accuracy: 0.5900\n",
      "Epoch 177/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6212 - accuracy: 0.5900\n",
      "Epoch 178/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6209 - accuracy: 0.5900\n",
      "Epoch 179/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6206 - accuracy: 0.5900\n",
      "Epoch 180/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6204 - accuracy: 0.5914\n",
      "Epoch 181/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6201 - accuracy: 0.5914\n",
      "Epoch 182/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6198 - accuracy: 0.5914\n",
      "Epoch 183/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6195 - accuracy: 0.5914\n",
      "Epoch 184/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6193 - accuracy: 0.5914\n",
      "Epoch 185/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6190 - accuracy: 0.5914\n",
      "Epoch 186/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6187 - accuracy: 0.5914\n",
      "Epoch 187/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6184 - accuracy: 0.5929\n",
      "Epoch 188/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6182 - accuracy: 0.5943\n",
      "Epoch 189/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6179 - accuracy: 0.5943\n",
      "Epoch 190/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6176 - accuracy: 0.5957\n",
      "Epoch 191/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6174 - accuracy: 0.5929\n",
      "Epoch 192/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6171 - accuracy: 0.5971\n",
      "Epoch 193/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6168 - accuracy: 0.5971\n",
      "Epoch 194/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6165 - accuracy: 0.5971\n",
      "Epoch 195/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6163 - accuracy: 0.5971\n",
      "Epoch 196/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6160 - accuracy: 0.5971\n",
      "Epoch 197/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6157 - accuracy: 0.5971\n",
      "Epoch 198/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6154 - accuracy: 0.6000\n",
      "Epoch 199/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6152 - accuracy: 0.5971\n",
      "Epoch 200/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6149 - accuracy: 0.6000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.67      0.80       131\n",
      "\n",
      "    accuracy                           0.67       131\n",
      "   macro avg       0.50      0.34      0.40       131\n",
      "weighted avg       1.00      0.67      0.80       131\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1)                 1875      \n",
      "=================================================================\n",
      "Total params: 1,875\n",
      "Trainable params: 1,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "22/22 [==============================] - 1s 2ms/step - loss: 0.6938 - accuracy: 0.4757\n",
      "Epoch 2/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.5843\n",
      "Epoch 3/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6887 - accuracy: 0.5957\n",
      "Epoch 4/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.5957\n",
      "Epoch 5/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6847 - accuracy: 0.5900\n",
      "Epoch 6/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6831 - accuracy: 0.5886\n",
      "Epoch 7/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6816 - accuracy: 0.5886\n",
      "Epoch 8/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6803 - accuracy: 0.5886\n",
      "Epoch 9/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6791 - accuracy: 0.5886\n",
      "Epoch 10/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6779 - accuracy: 0.5886\n",
      "Epoch 11/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6769 - accuracy: 0.5886\n",
      "Epoch 12/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6760 - accuracy: 0.5886\n",
      "Epoch 13/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6752 - accuracy: 0.5886\n",
      "Epoch 14/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6744 - accuracy: 0.5886\n",
      "Epoch 15/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6737 - accuracy: 0.5886\n",
      "Epoch 16/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6731 - accuracy: 0.5886\n",
      "Epoch 17/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6724 - accuracy: 0.5886\n",
      "Epoch 18/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6719 - accuracy: 0.5886\n",
      "Epoch 19/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6713 - accuracy: 0.5886\n",
      "Epoch 20/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6708 - accuracy: 0.5886\n",
      "Epoch 21/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6703 - accuracy: 0.5886\n",
      "Epoch 22/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6698 - accuracy: 0.5886\n",
      "Epoch 23/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6694 - accuracy: 0.5886\n",
      "Epoch 24/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6689 - accuracy: 0.5886\n",
      "Epoch 25/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6685 - accuracy: 0.5886\n",
      "Epoch 26/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6681 - accuracy: 0.5886\n",
      "Epoch 27/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6677 - accuracy: 0.5886\n",
      "Epoch 28/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6673 - accuracy: 0.5886\n",
      "Epoch 29/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6669 - accuracy: 0.5886\n",
      "Epoch 30/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6665 - accuracy: 0.5886\n",
      "Epoch 31/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6662 - accuracy: 0.5886\n",
      "Epoch 32/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6658 - accuracy: 0.5886\n",
      "Epoch 33/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6655 - accuracy: 0.5886\n",
      "Epoch 34/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6651 - accuracy: 0.5886\n",
      "Epoch 35/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6647 - accuracy: 0.5886\n",
      "Epoch 36/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6644 - accuracy: 0.5886\n",
      "Epoch 37/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6641 - accuracy: 0.5886\n",
      "Epoch 38/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6637 - accuracy: 0.5886\n",
      "Epoch 39/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6634 - accuracy: 0.5886\n",
      "Epoch 40/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6631 - accuracy: 0.5886\n",
      "Epoch 41/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6627 - accuracy: 0.5886\n",
      "Epoch 42/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6624 - accuracy: 0.5886\n",
      "Epoch 43/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6621 - accuracy: 0.5886\n",
      "Epoch 44/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6617 - accuracy: 0.5886\n",
      "Epoch 45/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6614 - accuracy: 0.5886\n",
      "Epoch 46/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6611 - accuracy: 0.5886\n",
      "Epoch 47/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6608 - accuracy: 0.5886\n",
      "Epoch 48/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6604 - accuracy: 0.5886\n",
      "Epoch 49/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6601 - accuracy: 0.5886\n",
      "Epoch 50/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6598 - accuracy: 0.5886\n",
      "Epoch 51/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6595 - accuracy: 0.5886\n",
      "Epoch 52/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6592 - accuracy: 0.5886\n",
      "Epoch 53/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6588 - accuracy: 0.5886\n",
      "Epoch 54/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6585 - accuracy: 0.5886\n",
      "Epoch 55/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6582 - accuracy: 0.5886\n",
      "Epoch 56/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6579 - accuracy: 0.5886\n",
      "Epoch 57/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6575 - accuracy: 0.5886\n",
      "Epoch 58/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6572 - accuracy: 0.5886\n",
      "Epoch 59/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6569 - accuracy: 0.5886\n",
      "Epoch 60/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6566 - accuracy: 0.5886\n",
      "Epoch 61/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6563 - accuracy: 0.5886\n",
      "Epoch 62/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6560 - accuracy: 0.5886\n",
      "Epoch 63/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6556 - accuracy: 0.5886\n",
      "Epoch 64/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6553 - accuracy: 0.5886\n",
      "Epoch 65/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6550 - accuracy: 0.5886\n",
      "Epoch 66/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6547 - accuracy: 0.5886\n",
      "Epoch 67/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6544 - accuracy: 0.5886\n",
      "Epoch 68/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6541 - accuracy: 0.5886\n",
      "Epoch 69/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6538 - accuracy: 0.5886\n",
      "Epoch 70/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6535 - accuracy: 0.5886\n",
      "Epoch 71/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6531 - accuracy: 0.5886\n",
      "Epoch 72/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6528 - accuracy: 0.5886\n",
      "Epoch 73/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6525 - accuracy: 0.5886\n",
      "Epoch 74/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6522 - accuracy: 0.5886\n",
      "Epoch 75/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6519 - accuracy: 0.5886\n",
      "Epoch 76/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6516 - accuracy: 0.5886\n",
      "Epoch 77/200\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6513 - accuracy: 0.5886\n",
      "Epoch 78/200\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6510 - accuracy: 0.5886 0s - loss: 0.6565 - accuracy: 0.57 - ETA: 0s - loss: 0.6573 - accuracy: 0.\n",
      "Epoch 79/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6507 - accuracy: 0.5886\n",
      "Epoch 80/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6503 - accuracy: 0.5886\n",
      "Epoch 81/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6500 - accuracy: 0.5886\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6497 - accuracy: 0.5886\n",
      "Epoch 83/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6495 - accuracy: 0.5886\n",
      "Epoch 84/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6491 - accuracy: 0.5886\n",
      "Epoch 85/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6488 - accuracy: 0.5886\n",
      "Epoch 86/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6485 - accuracy: 0.5886\n",
      "Epoch 87/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6482 - accuracy: 0.5886\n",
      "Epoch 88/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6479 - accuracy: 0.5886\n",
      "Epoch 89/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6476 - accuracy: 0.5886\n",
      "Epoch 90/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6473 - accuracy: 0.5886\n",
      "Epoch 91/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6470 - accuracy: 0.5886\n",
      "Epoch 92/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6467 - accuracy: 0.5886\n",
      "Epoch 93/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6464 - accuracy: 0.5886\n",
      "Epoch 94/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6460 - accuracy: 0.5886\n",
      "Epoch 95/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6457 - accuracy: 0.5886\n",
      "Epoch 96/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6454 - accuracy: 0.5886\n",
      "Epoch 97/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6451 - accuracy: 0.5886\n",
      "Epoch 98/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6449 - accuracy: 0.5886\n",
      "Epoch 99/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6445 - accuracy: 0.5886\n",
      "Epoch 100/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6442 - accuracy: 0.5886\n",
      "Epoch 101/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6440 - accuracy: 0.5886\n",
      "Epoch 102/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6436 - accuracy: 0.5886\n",
      "Epoch 103/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6433 - accuracy: 0.5886\n",
      "Epoch 104/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6430 - accuracy: 0.5886\n",
      "Epoch 105/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6427 - accuracy: 0.5886\n",
      "Epoch 106/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6425 - accuracy: 0.5886\n",
      "Epoch 107/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6421 - accuracy: 0.5886\n",
      "Epoch 108/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6418 - accuracy: 0.5886\n",
      "Epoch 109/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6416 - accuracy: 0.5886\n",
      "Epoch 110/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6412 - accuracy: 0.5886\n",
      "Epoch 111/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6409 - accuracy: 0.5886\n",
      "Epoch 112/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6406 - accuracy: 0.5886\n",
      "Epoch 113/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6404 - accuracy: 0.5886\n",
      "Epoch 114/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6401 - accuracy: 0.5886\n",
      "Epoch 115/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6397 - accuracy: 0.5886\n",
      "Epoch 116/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6395 - accuracy: 0.5886\n",
      "Epoch 117/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6391 - accuracy: 0.5886\n",
      "Epoch 118/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6389 - accuracy: 0.5886\n",
      "Epoch 119/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6386 - accuracy: 0.5886\n",
      "Epoch 120/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6383 - accuracy: 0.5886\n",
      "Epoch 121/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6380 - accuracy: 0.5886\n",
      "Epoch 122/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6377 - accuracy: 0.5886\n",
      "Epoch 123/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6374 - accuracy: 0.5886\n",
      "Epoch 124/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6371 - accuracy: 0.5886\n",
      "Epoch 125/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6368 - accuracy: 0.5886\n",
      "Epoch 126/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6365 - accuracy: 0.5886\n",
      "Epoch 127/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6362 - accuracy: 0.5886\n",
      "Epoch 128/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6359 - accuracy: 0.5886\n",
      "Epoch 129/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6356 - accuracy: 0.5886\n",
      "Epoch 130/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6353 - accuracy: 0.5886\n",
      "Epoch 131/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6350 - accuracy: 0.5886\n",
      "Epoch 132/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6348 - accuracy: 0.5886\n",
      "Epoch 133/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6345 - accuracy: 0.5886\n",
      "Epoch 134/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6342 - accuracy: 0.5886\n",
      "Epoch 135/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6339 - accuracy: 0.5886\n",
      "Epoch 136/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6336 - accuracy: 0.5886\n",
      "Epoch 137/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6333 - accuracy: 0.5886\n",
      "Epoch 138/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6330 - accuracy: 0.5886\n",
      "Epoch 139/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6327 - accuracy: 0.5886\n",
      "Epoch 140/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6324 - accuracy: 0.5886\n",
      "Epoch 141/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6321 - accuracy: 0.5886\n",
      "Epoch 142/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6319 - accuracy: 0.5886\n",
      "Epoch 143/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6316 - accuracy: 0.5886\n",
      "Epoch 144/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6313 - accuracy: 0.5886\n",
      "Epoch 145/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6310 - accuracy: 0.5886\n",
      "Epoch 146/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6307 - accuracy: 0.5886\n",
      "Epoch 147/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6304 - accuracy: 0.5886\n",
      "Epoch 148/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6301 - accuracy: 0.5900\n",
      "Epoch 149/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6298 - accuracy: 0.5900\n",
      "Epoch 150/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6296 - accuracy: 0.5900\n",
      "Epoch 151/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6293 - accuracy: 0.5900\n",
      "Epoch 152/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6290 - accuracy: 0.5900\n",
      "Epoch 153/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6287 - accuracy: 0.5900\n",
      "Epoch 154/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6284 - accuracy: 0.5900\n",
      "Epoch 155/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6281 - accuracy: 0.5900\n",
      "Epoch 156/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6279 - accuracy: 0.5900\n",
      "Epoch 157/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6276 - accuracy: 0.5900\n",
      "Epoch 158/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6273 - accuracy: 0.5900\n",
      "Epoch 159/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6270 - accuracy: 0.5900\n",
      "Epoch 160/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6267 - accuracy: 0.5900\n",
      "Epoch 161/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6264 - accuracy: 0.5900\n",
      "Epoch 162/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6261 - accuracy: 0.5900\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6258 - accuracy: 0.5900\n",
      "Epoch 164/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6256 - accuracy: 0.5900\n",
      "Epoch 165/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6253 - accuracy: 0.5900\n",
      "Epoch 166/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6250 - accuracy: 0.5900\n",
      "Epoch 167/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6248 - accuracy: 0.5900\n",
      "Epoch 168/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6245 - accuracy: 0.5900\n",
      "Epoch 169/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6242 - accuracy: 0.5900\n",
      "Epoch 170/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6239 - accuracy: 0.5900\n",
      "Epoch 171/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6236 - accuracy: 0.5900\n",
      "Epoch 172/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6234 - accuracy: 0.5900\n",
      "Epoch 173/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6231 - accuracy: 0.5900\n",
      "Epoch 174/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6228 - accuracy: 0.5900\n",
      "Epoch 175/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6225 - accuracy: 0.5900\n",
      "Epoch 176/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6222 - accuracy: 0.5900\n",
      "Epoch 177/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6220 - accuracy: 0.5900\n",
      "Epoch 178/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6217 - accuracy: 0.5914\n",
      "Epoch 179/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6214 - accuracy: 0.5914\n",
      "Epoch 180/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6211 - accuracy: 0.5914\n",
      "Epoch 181/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6208 - accuracy: 0.5914\n",
      "Epoch 182/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6206 - accuracy: 0.5914\n",
      "Epoch 183/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6203 - accuracy: 0.5914\n",
      "Epoch 184/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6200 - accuracy: 0.5914\n",
      "Epoch 185/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6197 - accuracy: 0.5914\n",
      "Epoch 186/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6195 - accuracy: 0.5914\n",
      "Epoch 187/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6192 - accuracy: 0.5914\n",
      "Epoch 188/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6189 - accuracy: 0.5914\n",
      "Epoch 189/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6186 - accuracy: 0.5929\n",
      "Epoch 190/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6184 - accuracy: 0.5914\n",
      "Epoch 191/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6181 - accuracy: 0.5929\n",
      "Epoch 192/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6178 - accuracy: 0.5929\n",
      "Epoch 193/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6176 - accuracy: 0.5943\n",
      "Epoch 194/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6173 - accuracy: 0.5957\n",
      "Epoch 195/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6170 - accuracy: 0.5957\n",
      "Epoch 196/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6167 - accuracy: 0.5971\n",
      "Epoch 197/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6165 - accuracy: 0.5971\n",
      "Epoch 198/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6162 - accuracy: 0.5986\n",
      "Epoch 199/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6159 - accuracy: 0.5986\n",
      "Epoch 200/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6157 - accuracy: 0.5986\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.02      1.00      0.05         1\n",
      "        True       1.00      0.68      0.81       130\n",
      "\n",
      "    accuracy                           0.68       131\n",
      "   macro avg       0.51      0.84      0.43       131\n",
      "weighted avg       0.99      0.68      0.80       131\n",
      "\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1)                 1875      \n",
      "=================================================================\n",
      "Total params: 1,875\n",
      "Trainable params: 1,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "22/22 [==============================] - 1s 2ms/step - loss: 0.6905 - accuracy: 0.5800\n",
      "Epoch 2/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6886 - accuracy: 0.5857\n",
      "Epoch 3/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6870 - accuracy: 0.5886\n",
      "Epoch 4/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6855 - accuracy: 0.5886\n",
      "Epoch 5/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6841 - accuracy: 0.5886\n",
      "Epoch 6/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6829 - accuracy: 0.5886\n",
      "Epoch 7/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6818 - accuracy: 0.5886\n",
      "Epoch 8/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6808 - accuracy: 0.5886\n",
      "Epoch 9/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6799 - accuracy: 0.5886\n",
      "Epoch 10/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6791 - accuracy: 0.5886\n",
      "Epoch 11/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6783 - accuracy: 0.5886\n",
      "Epoch 12/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6776 - accuracy: 0.5886\n",
      "Epoch 13/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6769 - accuracy: 0.5886\n",
      "Epoch 14/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.6763 - accuracy: 0.5886\n",
      "Epoch 15/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.6757 - accuracy: 0.5886\n",
      "Epoch 16/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6752 - accuracy: 0.5886\n",
      "Epoch 17/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6747 - accuracy: 0.5886\n",
      "Epoch 18/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6742 - accuracy: 0.5886\n",
      "Epoch 19/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6737 - accuracy: 0.5886\n",
      "Epoch 20/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6733 - accuracy: 0.5886\n",
      "Epoch 21/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6728 - accuracy: 0.5886\n",
      "Epoch 22/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6724 - accuracy: 0.5886\n",
      "Epoch 23/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6720 - accuracy: 0.5886\n",
      "Epoch 24/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6715 - accuracy: 0.5886\n",
      "Epoch 25/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6711 - accuracy: 0.5886\n",
      "Epoch 26/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6708 - accuracy: 0.5886\n",
      "Epoch 27/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6704 - accuracy: 0.5886\n",
      "Epoch 28/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6700 - accuracy: 0.5886\n",
      "Epoch 29/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6696 - accuracy: 0.5886\n",
      "Epoch 30/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6693 - accuracy: 0.5886\n",
      "Epoch 31/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6689 - accuracy: 0.5886\n",
      "Epoch 32/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6686 - accuracy: 0.5886\n",
      "Epoch 33/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6682 - accuracy: 0.5886\n",
      "Epoch 34/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6679 - accuracy: 0.5886\n",
      "Epoch 35/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6675 - accuracy: 0.5886\n",
      "Epoch 36/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6672 - accuracy: 0.5886\n",
      "Epoch 37/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6668 - accuracy: 0.5886\n",
      "Epoch 38/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6665 - accuracy: 0.5886\n",
      "Epoch 39/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6662 - accuracy: 0.5886\n",
      "Epoch 40/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6658 - accuracy: 0.5886\n",
      "Epoch 41/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6655 - accuracy: 0.5886\n",
      "Epoch 42/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6652 - accuracy: 0.5886\n",
      "Epoch 43/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6648 - accuracy: 0.5886\n",
      "Epoch 44/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6645 - accuracy: 0.5886\n",
      "Epoch 45/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6642 - accuracy: 0.5886\n",
      "Epoch 46/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6639 - accuracy: 0.5886\n",
      "Epoch 47/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6635 - accuracy: 0.5886\n",
      "Epoch 48/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6632 - accuracy: 0.5886\n",
      "Epoch 49/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6629 - accuracy: 0.5886\n",
      "Epoch 50/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6625 - accuracy: 0.5886\n",
      "Epoch 51/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6622 - accuracy: 0.5886\n",
      "Epoch 52/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6619 - accuracy: 0.5886\n",
      "Epoch 53/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6616 - accuracy: 0.5886\n",
      "Epoch 54/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6612 - accuracy: 0.5886\n",
      "Epoch 55/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6609 - accuracy: 0.5886\n",
      "Epoch 56/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6606 - accuracy: 0.5886\n",
      "Epoch 57/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6603 - accuracy: 0.5886\n",
      "Epoch 58/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6599 - accuracy: 0.5886\n",
      "Epoch 59/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6596 - accuracy: 0.5886\n",
      "Epoch 60/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6593 - accuracy: 0.5886\n",
      "Epoch 61/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6590 - accuracy: 0.5886\n",
      "Epoch 62/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6587 - accuracy: 0.5886\n",
      "Epoch 63/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6584 - accuracy: 0.5886\n",
      "Epoch 64/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6580 - accuracy: 0.5886\n",
      "Epoch 65/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6577 - accuracy: 0.5886\n",
      "Epoch 66/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6574 - accuracy: 0.5886\n",
      "Epoch 67/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6571 - accuracy: 0.5886\n",
      "Epoch 68/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6568 - accuracy: 0.5886\n",
      "Epoch 69/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6565 - accuracy: 0.5886\n",
      "Epoch 70/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6561 - accuracy: 0.5886\n",
      "Epoch 71/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6558 - accuracy: 0.5886\n",
      "Epoch 72/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6555 - accuracy: 0.5886\n",
      "Epoch 73/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6552 - accuracy: 0.5886\n",
      "Epoch 74/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6549 - accuracy: 0.5886\n",
      "Epoch 75/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6546 - accuracy: 0.5886\n",
      "Epoch 76/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6543 - accuracy: 0.5886\n",
      "Epoch 77/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6539 - accuracy: 0.5886\n",
      "Epoch 78/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6536 - accuracy: 0.5886\n",
      "Epoch 79/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6533 - accuracy: 0.5886\n",
      "Epoch 80/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6530 - accuracy: 0.5886\n",
      "Epoch 81/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6527 - accuracy: 0.5886\n",
      "Epoch 82/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6524 - accuracy: 0.5886\n",
      "Epoch 83/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6521 - accuracy: 0.5886\n",
      "Epoch 84/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6517 - accuracy: 0.5886\n",
      "Epoch 85/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6514 - accuracy: 0.5886\n",
      "Epoch 86/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6512 - accuracy: 0.5886\n",
      "Epoch 87/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6508 - accuracy: 0.5886\n",
      "Epoch 88/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6505 - accuracy: 0.5886\n",
      "Epoch 89/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6502 - accuracy: 0.5886\n",
      "Epoch 90/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6499 - accuracy: 0.5886\n",
      "Epoch 91/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6496 - accuracy: 0.5886\n",
      "Epoch 92/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6493 - accuracy: 0.5886\n",
      "Epoch 93/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6490 - accuracy: 0.5886\n",
      "Epoch 94/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6487 - accuracy: 0.5886\n",
      "Epoch 95/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6484 - accuracy: 0.5886\n",
      "Epoch 96/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6480 - accuracy: 0.5886\n",
      "Epoch 97/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6477 - accuracy: 0.5886\n",
      "Epoch 98/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6475 - accuracy: 0.5886\n",
      "Epoch 99/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6471 - accuracy: 0.5886\n",
      "Epoch 100/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6468 - accuracy: 0.5886\n",
      "Epoch 101/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6465 - accuracy: 0.5886\n",
      "Epoch 102/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6462 - accuracy: 0.5886\n",
      "Epoch 103/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6459 - accuracy: 0.5886\n",
      "Epoch 104/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6456 - accuracy: 0.5886\n",
      "Epoch 105/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6453 - accuracy: 0.5886\n",
      "Epoch 106/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6450 - accuracy: 0.5886\n",
      "Epoch 107/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6447 - accuracy: 0.5886\n",
      "Epoch 108/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6444 - accuracy: 0.5886\n",
      "Epoch 109/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6441 - accuracy: 0.5886\n",
      "Epoch 110/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6438 - accuracy: 0.5886\n",
      "Epoch 111/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6435 - accuracy: 0.5886\n",
      "Epoch 112/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6432 - accuracy: 0.5886\n",
      "Epoch 113/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.5886\n",
      "Epoch 114/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6426 - accuracy: 0.5886\n",
      "Epoch 115/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6423 - accuracy: 0.5886\n",
      "Epoch 116/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6420 - accuracy: 0.5886\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6417 - accuracy: 0.5886\n",
      "Epoch 118/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6414 - accuracy: 0.5886\n",
      "Epoch 119/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6411 - accuracy: 0.5886\n",
      "Epoch 120/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6408 - accuracy: 0.5886\n",
      "Epoch 121/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6405 - accuracy: 0.5886\n",
      "Epoch 122/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6402 - accuracy: 0.5886\n",
      "Epoch 123/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6399 - accuracy: 0.5886\n",
      "Epoch 124/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6396 - accuracy: 0.5886\n",
      "Epoch 125/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6393 - accuracy: 0.5886\n",
      "Epoch 126/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6390 - accuracy: 0.5886\n",
      "Epoch 127/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6387 - accuracy: 0.5886\n",
      "Epoch 128/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6384 - accuracy: 0.5886\n",
      "Epoch 129/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6381 - accuracy: 0.5886\n",
      "Epoch 130/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6378 - accuracy: 0.5886\n",
      "Epoch 131/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6375 - accuracy: 0.5886\n",
      "Epoch 132/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6372 - accuracy: 0.5886\n",
      "Epoch 133/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6369 - accuracy: 0.5886\n",
      "Epoch 134/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6366 - accuracy: 0.5886\n",
      "Epoch 135/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6363 - accuracy: 0.5886\n",
      "Epoch 136/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6360 - accuracy: 0.5886\n",
      "Epoch 137/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6358 - accuracy: 0.5886\n",
      "Epoch 138/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6355 - accuracy: 0.5886\n",
      "Epoch 139/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6352 - accuracy: 0.5886\n",
      "Epoch 140/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6349 - accuracy: 0.5886\n",
      "Epoch 141/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6346 - accuracy: 0.5886\n",
      "Epoch 142/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6343 - accuracy: 0.5886\n",
      "Epoch 143/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6340 - accuracy: 0.5886\n",
      "Epoch 144/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6337 - accuracy: 0.5886\n",
      "Epoch 145/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6334 - accuracy: 0.5886\n",
      "Epoch 146/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6331 - accuracy: 0.5886\n",
      "Epoch 147/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6328 - accuracy: 0.5886\n",
      "Epoch 148/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6326 - accuracy: 0.5886\n",
      "Epoch 149/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6323 - accuracy: 0.5886\n",
      "Epoch 150/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6320 - accuracy: 0.5886\n",
      "Epoch 151/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6317 - accuracy: 0.5886\n",
      "Epoch 152/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6314 - accuracy: 0.5886\n",
      "Epoch 153/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6311 - accuracy: 0.5886\n",
      "Epoch 154/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6308 - accuracy: 0.5886\n",
      "Epoch 155/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6305 - accuracy: 0.5886\n",
      "Epoch 156/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6302 - accuracy: 0.5886\n",
      "Epoch 157/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6300 - accuracy: 0.5886\n",
      "Epoch 158/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6297 - accuracy: 0.5886\n",
      "Epoch 159/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6294 - accuracy: 0.5886\n",
      "Epoch 160/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6291 - accuracy: 0.5886\n",
      "Epoch 161/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6288 - accuracy: 0.5886\n",
      "Epoch 162/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6285 - accuracy: 0.5886\n",
      "Epoch 163/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6283 - accuracy: 0.5886\n",
      "Epoch 164/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6280 - accuracy: 0.5886\n",
      "Epoch 165/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6277 - accuracy: 0.5886\n",
      "Epoch 166/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6274 - accuracy: 0.5886\n",
      "Epoch 167/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6271 - accuracy: 0.5886\n",
      "Epoch 168/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6268 - accuracy: 0.5886\n",
      "Epoch 169/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6265 - accuracy: 0.5886\n",
      "Epoch 170/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6263 - accuracy: 0.5886\n",
      "Epoch 171/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6260 - accuracy: 0.5886\n",
      "Epoch 172/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6257 - accuracy: 0.5886\n",
      "Epoch 173/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6254 - accuracy: 0.5886\n",
      "Epoch 174/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6251 - accuracy: 0.5886\n",
      "Epoch 175/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6248 - accuracy: 0.5886\n",
      "Epoch 176/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6246 - accuracy: 0.5886\n",
      "Epoch 177/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6243 - accuracy: 0.5886\n",
      "Epoch 178/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6240 - accuracy: 0.5886\n",
      "Epoch 179/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6237 - accuracy: 0.5886\n",
      "Epoch 180/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6234 - accuracy: 0.5886\n",
      "Epoch 181/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6232 - accuracy: 0.5886\n",
      "Epoch 182/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6229 - accuracy: 0.5900\n",
      "Epoch 183/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6226 - accuracy: 0.5914\n",
      "Epoch 184/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6223 - accuracy: 0.5929\n",
      "Epoch 185/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6220 - accuracy: 0.5929\n",
      "Epoch 186/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6218 - accuracy: 0.5929\n",
      "Epoch 187/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6215 - accuracy: 0.5929\n",
      "Epoch 188/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6212 - accuracy: 0.5929\n",
      "Epoch 189/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6210 - accuracy: 0.5929\n",
      "Epoch 190/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6207 - accuracy: 0.5929\n",
      "Epoch 191/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6204 - accuracy: 0.5943\n",
      "Epoch 192/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6201 - accuracy: 0.5957\n",
      "Epoch 193/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6199 - accuracy: 0.5943\n",
      "Epoch 194/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6196 - accuracy: 0.5943\n",
      "Epoch 195/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.6193 - accuracy: 0.5957\n",
      "Epoch 196/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6190 - accuracy: 0.5957\n",
      "Epoch 197/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6187 - accuracy: 0.5957\n",
      "Epoch 198/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6185 - accuracy: 0.5957\n",
      "Epoch 199/200\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6182 - accuracy: 0.5957\n",
      "Epoch 200/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6179 - accuracy: 0.5957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.02      1.00      0.05         1\n",
      "        True       1.00      0.68      0.81       130\n",
      "\n",
      "    accuracy                           0.68       131\n",
      "   macro avg       0.51      0.84      0.43       131\n",
      "weighted avg       0.99      0.68      0.80       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for i in range(n_times):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(units=1, activation='sigmoid', input_shape=(1874, )))\n",
    "    model.summary()\n",
    "    model.compile(optimizer='Sgd', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "    epochs_hist = model.fit(train_X1, train_y1, epochs = 200) \n",
    "    predict = model.predict(test_x1)\n",
    "    y_predict = (predict > 0.5)\n",
    "    print(classification_report(y_predict, test_y1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a66a4a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD  1\n",
      "==========completed\n",
      "FOLD  2\n",
      "==========completed\n",
      "FOLD  3\n",
      "==========completed\n",
      "FOLD  4\n",
      "==========completed\n",
      "FOLD  5\n",
      "==========completed\n",
      "          P1,   P2, All, P1&P2\n",
      "LR:      [0.85 0.83 0.86 0.87]\n",
      "RF-50:   [0.82 0.82 0.87 0.86]\n",
      "RF-100:  [0.82 0.84 0.87 0.87]\n",
      "RF-200:  [0.82 0.84 0.88 0.87]\n",
      "RF-400:  [0.82 0.85 0.88 0.87]\n"
     ]
    }
   ],
   "source": [
    "# For each method, these will hold 4 accuracy results for each of 5 folds\n",
    "# (1) accuracy of model trained on data from P1\n",
    "# (2) accuracy of model trained on data from P2\n",
    "# (3) accuracy of model trained on data from P1 and from P2\n",
    "# (4) accuracy of aggregation of model (1) and (2) from above\n",
    "LRresults = np.zeros((5, 4))\n",
    "RF50results = np.zeros((5, 4))\n",
    "RF100results = np.zeros((5, 4))\n",
    "RF200results = np.zeros((5, 4))\n",
    "RF400results = np.zeros((5, 4))\n",
    "\n",
    "kf1 = KFold(n_splits=5,shuffle = True,random_state = 42)\n",
    "kf2 = KFold(n_splits=5,shuffle = True,random_state = 42)\n",
    "\n",
    "\n",
    "i = 0\n",
    "for result1,result2 in zip(kf1.split(X1,y1),kf2.split(X2,y2)):\n",
    "  print(\"FOLD \", i+1)\n",
    "  X1_train, X1_test = X1.iloc[result1[0]], X1.iloc[result1[1]]\n",
    "  y1_train, y1_test = y1[result1[0]], y1[result1[1]]\n",
    "  X2_train, X2_test = X2.iloc[result2[0]], X2.iloc[result2[1]]\n",
    "  y2_train, y2_test = y2[result2[0]], y2[result2[1]]\n",
    "\n",
    "  X_train = X1_train.append(X2_train)\n",
    "  y_train = np.append(y1_train,y2_train)\n",
    "  X_test = X1_test.append(X2_test)\n",
    "  y_test = np.append(y1_test,y2_test)\n",
    "\n",
    "\n",
    "  ########## Train and test logistic regression models #################\n",
    "\n",
    "  clf1 = LogisticRegression(solver='liblinear',random_state=10000)\n",
    "  clf2 = LogisticRegression(solver='liblinear',random_state=10000)\n",
    "  clf = LogisticRegression(solver='liblinear',random_state=10000)\n",
    "  \n",
    "  clf1.fit(X1_train, y1_train)\n",
    "  accP1  = accuracy_score(y_test,clf1.predict(X_test))\n",
    "  \n",
    "  clf2.fit(X2_train, y2_train)\n",
    "  accP2 = accuracy_score(y_test,clf2.predict(X_test))\n",
    "    \n",
    "  clf.fit(X_train, y_train)\n",
    "  accALL = accuracy_score(y_test,clf.predict(X_test))\n",
    "  \n",
    "  # Merging of LR models\n",
    "  clf1.coef_ = (clf1.coef_ + clf2.coef_)/2\n",
    "  clf1.intercept_ = (clf1.intercept_ + clf2.intercept_)/2\n",
    "  accMERG = accuracy_score(y_test,clf1.predict(X_test))  \n",
    "\n",
    "  LRresults[i] = [accP1,accP2,accALL,accMERG]\n",
    "    \n",
    "\n",
    "  ########## Train and test RF models #################################\n",
    "\n",
    "  ntrees=50\n",
    "  RFresults=RF50results\n",
    "  evaluateForest(ntrees, RFresults, X_train, y_train, X1_train, y1_train, \n",
    "                         X2_train, y2_train, X_test, y_test) \n",
    "  \n",
    "  ntrees=100\n",
    "  RFresults=RF100results\n",
    "  evaluateForest(ntrees, RFresults, X_train, y_train, X1_train, y1_train, \n",
    "                         X2_train, y2_train, X_test, y_test) \n",
    "\n",
    "  ntrees=200\n",
    "  RFresults=RF200results\n",
    "  evaluateForest(ntrees, RFresults, X_train, y_train, X1_train, y1_train, \n",
    "                         X2_train, y2_train, X_test, y_test) \n",
    "\n",
    "  ntrees=400\n",
    "  RFresults=RF400results\n",
    "  evaluateForest(ntrees, RFresults, X_train, y_train, X1_train, y1_train, \n",
    "                         X2_train, y2_train, X_test, y_test) \n",
    "\n",
    "\n",
    "  print(\"==========completed\")\n",
    "  i = i + 1\n",
    "\n",
    "# Printing the averages over the 5 folds\n",
    "print(\"          P1,   P2, All, P1&P2\")\n",
    "np.set_printoptions(precision=2)\n",
    "print(\"LR:     \",np.mean(LRresults, axis=0))\n",
    "print(\"RF-50:  \",np.mean(RF50results, axis=0))\n",
    "print(\"RF-100: \",np.mean(RF100results, axis=0))\n",
    "print(\"RF-200: \",np.mean(RF200results, axis=0))\n",
    "print(\"RF-400: \",np.mean(RF400results, axis=0)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
