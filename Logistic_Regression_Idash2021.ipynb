{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9454b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc33305",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "\n",
    "def preprocess(dirty_df):\n",
    "  dirty_df = dirty_df.drop(['patient_id','cohort_type'], axis = 1)\n",
    "  target_map = {u'1': 1, u'0': 0}\n",
    "  dirty_df['__target__'] = dirty_df['cohort_flag'].map(str).map(target_map)\n",
    "  dirty_df = dirty_df.drop(['cohort_flag'], axis = 1)\n",
    "  clean_X = dirty_df.drop('__target__', axis=1)\n",
    "  clean_y = np.array(dirty_df['__target__'])\n",
    "  return clean_X, clean_y\n",
    "\n",
    "def evaluateForest(ntrees, RFresults, X_train, y_train, X1_train, y1_train, X2_train, y2_train, X_test, y_test): \n",
    "  clf1 = RandomForestClassifier(n_estimators=ntrees,random_state=10000)\n",
    "  clf2 = RandomForestClassifier(n_estimators=ntrees,random_state=10000)\n",
    "  clf  = RandomForestClassifier(n_estimators=ntrees,random_state=10000)\n",
    "  \n",
    "  clf1.fit(X1_train, y1_train)\n",
    "  accP1  = accuracy_score(y_test,clf1.predict(X_test))\n",
    "\n",
    "  clf2.fit(X2_train, y2_train)\n",
    "  accP2 = accuracy_score(y_test,clf2.predict(X_test))\n",
    "    \n",
    "  clf.fit(X_train, y_train)\n",
    "  accALL = accuracy_score(y_test,clf.predict(X_test))\n",
    "  \n",
    "  # Merging of RF models  \n",
    "  clf1.estimators_ += clf2.estimators_\n",
    "  clf1.n_estimators = len(clf1.estimators_)\n",
    "  accMERG = accuracy_score(y_test,clf1.predict(X_test))\n",
    "\n",
    "  RFresults[i] = [accP1,accP2,accALL,accMERG]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bc701ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "\n",
    "# Load the data\n",
    "df1 = pd.read_csv('party_1.csv')\n",
    "df2 = pd.read_csv('party_2.csv')\n",
    "\n",
    "X1, y1 = preprocess(df1)\n",
    "X2, y2 = preprocess(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e07b8d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# party1\n",
    "train_y1=y1[0:n_train]\n",
    "train_X1=X1[0:n_train]\n",
    "test_y1=y1[n_train:n_train+n_test]\n",
    "test_x1=X1[n_train:n_train+n_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0ddebc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_mpc(list_files, out_file_mpc, n_train, n_test):\n",
    "    # part\n",
    "    train_y1=y1[0:n_train]\n",
    "    train_X1=X1[0:n_train]\n",
    "    test_y1=y1[n_train:n_train+n_test]\n",
    "    test_x1=X1[n_train:n_train+n_test]    \n",
    "    \n",
    "    train_X1.to_csv(list_files[0], header=False, sep=' ', index=False)\n",
    "    y1_aux = pd.DataFrame(train_y1)\n",
    "    y1_aux.to_csv(list_files[1], header=False, sep=' ', index=False)\n",
    "    test_x1.to_csv(list_files[2], header=False, sep=' ', index=False)\n",
    "    y2_aux = pd.DataFrame(test_y1)\n",
    "    y2_aux.to_csv(list_files[3], header=False, sep=' ', index=False)\n",
    "    \n",
    "    \n",
    "    filenames = list_files\n",
    "    with open(out_file_mpc, 'w') as outfile:\n",
    "        for fname in filenames:\n",
    "            with open(fname) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "                    \n",
    "generate_input_mpc(['Player-Data/train_X1', 'Player-Data/train_y1', 'Player-Data/test_x1', 'Player-Data/test_y1'], 'Player-Data/Input-P0-0', n_train=700, n_test=131)\n",
    "\n",
    "generate_input_mpc(['Player-Data/train_X2', 'Player-Data/train_y2', 'Player-Data/test_x2', 'Player-Data/test_y2'], 'Player-Data/Input-P1-0', n_train=700, n_test=131)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9357ac8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix\n",
      " [[36  6]\n",
      " [ 7 82]]\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85        42\n",
      "           1       0.93      0.92      0.93        89\n",
      "\n",
      "    accuracy                           0.90       131\n",
      "   macro avg       0.88      0.89      0.89       131\n",
      "weighted avg       0.90      0.90      0.90       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "perceptron_sklearn = Perceptron(max_iter=200, random_state=100)\n",
    "perceptron_sklearn.fit(train_X1,train_y1)\n",
    "\n",
    "resultado_predicao = perceptron_sklearn.predict(test_x1)\n",
    "\n",
    "print('confusion_matrix\\n', confusion_matrix(resultado_predicao, test_y1))\n",
    "print('classification_report\\n',classification_report(resultado_predicao, test_y1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "71b0e89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix\n",
      " [[33  6]\n",
      " [10 82]]\n",
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.80        39\n",
      "           1       0.93      0.89      0.91        92\n",
      "\n",
      "    accuracy                           0.88       131\n",
      "   macro avg       0.85      0.87      0.86       131\n",
      "weighted avg       0.88      0.88      0.88       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "regr = LogisticRegression(max_iter=200, random_state=100)\n",
    "regr.fit(train_X1,train_y1)\n",
    "resultado_predicao = regr.predict(test_x1)\n",
    "print('confusion_matrix\\n', confusion_matrix(resultado_predicao, test_y1))\n",
    "print('classification_report\\n',classification_report(resultado_predicao, test_y1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "534028fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 1)                 1875      \n",
      "=================================================================\n",
      "Total params: 1,875\n",
      "Trainable params: 1,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "22/22 [==============================] - 4s 6ms/step - loss: 0.6879 - accuracy: 0.5529\n",
      "Epoch 2/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.6224 - accuracy: 0.6471\n",
      "Epoch 3/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5827 - accuracy: 0.6929\n",
      "Epoch 4/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5489 - accuracy: 0.7443\n",
      "Epoch 5/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.5205 - accuracy: 0.7771\n",
      "Epoch 6/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4953 - accuracy: 0.8114\n",
      "Epoch 7/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.4742 - accuracy: 0.8200: 0s - loss: 0.4742 - accuracy: 0.82\n",
      "Epoch 8/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4548 - accuracy: 0.8400\n",
      "Epoch 9/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4379 - accuracy: 0.8471\n",
      "Epoch 10/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.4227 - accuracy: 0.8629\n",
      "Epoch 11/200\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4115 - accuracy: 0.87 - 0s 3ms/step - loss: 0.4088 - accuracy: 0.8757\n",
      "Epoch 12/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3964 - accuracy: 0.8814\n",
      "Epoch 13/200\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.3849 - accuracy: 0.8886\n",
      "Epoch 14/200\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.3737 - accuracy: 0.8843\n",
      "Epoch 15/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.3641 - accuracy: 0.8986\n",
      "Epoch 16/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3551 - accuracy: 0.9014\n",
      "Epoch 17/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.3468 - accuracy: 0.9029\n",
      "Epoch 18/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3391 - accuracy: 0.9114\n",
      "Epoch 19/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3319 - accuracy: 0.9114\n",
      "Epoch 20/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3245 - accuracy: 0.9143\n",
      "Epoch 21/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3180 - accuracy: 0.9200\n",
      "Epoch 22/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3124 - accuracy: 0.9171\n",
      "Epoch 23/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.3063 - accuracy: 0.9200\n",
      "Epoch 24/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3010 - accuracy: 0.9200\n",
      "Epoch 25/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2956 - accuracy: 0.9257\n",
      "Epoch 26/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.2909 - accuracy: 0.9229\n",
      "Epoch 27/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.2859 - accuracy: 0.9243\n",
      "Epoch 28/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.2813 - accuracy: 0.9314: 0s - loss: 0.2644 - accuracy: 0.\n",
      "Epoch 29/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2771 - accuracy: 0.9286\n",
      "Epoch 30/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.2727 - accuracy: 0.9314\n",
      "Epoch 31/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2692 - accuracy: 0.9357\n",
      "Epoch 32/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.2648 - accuracy: 0.9329\n",
      "Epoch 33/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2614 - accuracy: 0.9357\n",
      "Epoch 34/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.2580 - accuracy: 0.9343\n",
      "Epoch 35/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2546 - accuracy: 0.9357\n",
      "Epoch 36/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2512 - accuracy: 0.9371\n",
      "Epoch 37/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.2483 - accuracy: 0.9386\n",
      "Epoch 38/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2450 - accuracy: 0.9371\n",
      "Epoch 39/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2422 - accuracy: 0.9386\n",
      "Epoch 40/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.2393 - accuracy: 0.9400\n",
      "Epoch 41/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2365 - accuracy: 0.9429\n",
      "Epoch 42/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2338 - accuracy: 0.9443\n",
      "Epoch 43/200\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.2312 - accuracy: 0.9429\n",
      "Epoch 44/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2288 - accuracy: 0.9443\n",
      "Epoch 45/200\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.2264 - accuracy: 0.9429\n",
      "Epoch 46/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.9471\n",
      "Epoch 47/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2215 - accuracy: 0.9486\n",
      "Epoch 48/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.2192 - accuracy: 0.9486\n",
      "Epoch 49/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2171 - accuracy: 0.9514\n",
      "Epoch 50/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.2151 - accuracy: 0.9514\n",
      "Epoch 51/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.2128 - accuracy: 0.9500\n",
      "Epoch 52/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.2110 - accuracy: 0.9529\n",
      "Epoch 53/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2089 - accuracy: 0.9514\n",
      "Epoch 54/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.2066 - accuracy: 0.9543\n",
      "Epoch 55/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.2051 - accuracy: 0.9514\n",
      "Epoch 56/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.2030 - accuracy: 0.9557\n",
      "Epoch 57/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2013 - accuracy: 0.9571\n",
      "Epoch 58/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1997 - accuracy: 0.9543\n",
      "Epoch 59/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1978 - accuracy: 0.9557\n",
      "Epoch 60/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1962 - accuracy: 0.9571\n",
      "Epoch 61/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1945 - accuracy: 0.9571\n",
      "Epoch 62/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1928 - accuracy: 0.9571\n",
      "Epoch 63/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1912 - accuracy: 0.9586\n",
      "Epoch 64/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1899 - accuracy: 0.9614\n",
      "Epoch 65/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.1882 - accuracy: 0.9614\n",
      "Epoch 66/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1869 - accuracy: 0.9614\n",
      "Epoch 67/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.1852 - accuracy: 0.9629\n",
      "Epoch 68/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1837 - accuracy: 0.9643\n",
      "Epoch 69/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1823 - accuracy: 0.9643\n",
      "Epoch 70/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1810 - accuracy: 0.9643\n",
      "Epoch 71/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.1796 - accuracy: 0.9657: 0s - loss: 0.1762 - accuracy: 0.\n",
      "Epoch 72/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1783 - accuracy: 0.9671\n",
      "Epoch 73/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1770 - accuracy: 0.9643\n",
      "Epoch 74/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1757 - accuracy: 0.9671\n",
      "Epoch 75/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1744 - accuracy: 0.9714\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 7ms/step - loss: 0.1734 - accuracy: 0.9686\n",
      "Epoch 77/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1720 - accuracy: 0.9700\n",
      "Epoch 78/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1708 - accuracy: 0.9671\n",
      "Epoch 79/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1696 - accuracy: 0.9714\n",
      "Epoch 80/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1685 - accuracy: 0.9743\n",
      "Epoch 81/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1673 - accuracy: 0.9729\n",
      "Epoch 82/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.1662 - accuracy: 0.9743\n",
      "Epoch 83/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1650 - accuracy: 0.9714\n",
      "Epoch 84/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1639 - accuracy: 0.9714\n",
      "Epoch 85/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1630 - accuracy: 0.9729\n",
      "Epoch 86/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1620 - accuracy: 0.9729\n",
      "Epoch 87/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.1608 - accuracy: 0.9729: 0s - loss: 0.1609 - accuracy: 0.97\n",
      "Epoch 88/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1598 - accuracy: 0.9743\n",
      "Epoch 89/200\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.1588 - accuracy: 0.9771\n",
      "Epoch 90/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1578 - accuracy: 0.9757\n",
      "Epoch 91/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1569 - accuracy: 0.9771\n",
      "Epoch 92/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1558 - accuracy: 0.9771\n",
      "Epoch 93/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1549 - accuracy: 0.9771\n",
      "Epoch 94/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1539 - accuracy: 0.9786\n",
      "Epoch 95/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1530 - accuracy: 0.9786\n",
      "Epoch 96/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1521 - accuracy: 0.9786\n",
      "Epoch 97/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1513 - accuracy: 0.9786\n",
      "Epoch 98/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1503 - accuracy: 0.9786\n",
      "Epoch 99/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1495 - accuracy: 0.9786\n",
      "Epoch 100/200\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1421 - accuracy: 0.98 - 0s 5ms/step - loss: 0.1485 - accuracy: 0.9786\n",
      "Epoch 101/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1478 - accuracy: 0.9786\n",
      "Epoch 102/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1469 - accuracy: 0.9786\n",
      "Epoch 103/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1461 - accuracy: 0.9800\n",
      "Epoch 104/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1454 - accuracy: 0.9786\n",
      "Epoch 105/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1443 - accuracy: 0.9800\n",
      "Epoch 106/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1436 - accuracy: 0.9786\n",
      "Epoch 107/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1430 - accuracy: 0.9786\n",
      "Epoch 108/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1422 - accuracy: 0.9786\n",
      "Epoch 109/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1414 - accuracy: 0.9800\n",
      "Epoch 110/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1405 - accuracy: 0.9800\n",
      "Epoch 111/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1397 - accuracy: 0.9800\n",
      "Epoch 112/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1391 - accuracy: 0.9800\n",
      "Epoch 113/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1383 - accuracy: 0.9800\n",
      "Epoch 114/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1375 - accuracy: 0.9800\n",
      "Epoch 115/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1369 - accuracy: 0.9814\n",
      "Epoch 116/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1361 - accuracy: 0.9814\n",
      "Epoch 117/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1354 - accuracy: 0.9800\n",
      "Epoch 118/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1348 - accuracy: 0.9800\n",
      "Epoch 119/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1341 - accuracy: 0.9814\n",
      "Epoch 120/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1334 - accuracy: 0.9829\n",
      "Epoch 121/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1327 - accuracy: 0.9814\n",
      "Epoch 122/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1321 - accuracy: 0.9814\n",
      "Epoch 123/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1315 - accuracy: 0.9814\n",
      "Epoch 124/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1309 - accuracy: 0.9814\n",
      "Epoch 125/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1302 - accuracy: 0.9814\n",
      "Epoch 126/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.1294 - accuracy: 0.9843\n",
      "Epoch 127/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1289 - accuracy: 0.9829\n",
      "Epoch 128/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1282 - accuracy: 0.9843\n",
      "Epoch 129/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1276 - accuracy: 0.9843\n",
      "Epoch 130/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1270 - accuracy: 0.9829\n",
      "Epoch 131/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1265 - accuracy: 0.9843\n",
      "Epoch 132/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1259 - accuracy: 0.9843\n",
      "Epoch 133/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.1252 - accuracy: 0.9843\n",
      "Epoch 134/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.1248 - accuracy: 0.9857\n",
      "Epoch 135/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1241 - accuracy: 0.9843\n",
      "Epoch 136/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.1235 - accuracy: 0.9843\n",
      "Epoch 137/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1230 - accuracy: 0.9843\n",
      "Epoch 138/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.1224 - accuracy: 0.9857\n",
      "Epoch 139/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1218 - accuracy: 0.9857\n",
      "Epoch 140/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.1213 - accuracy: 0.9871: 0s - loss: 0.1470 - accuracy: \n",
      "Epoch 141/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.1208 - accuracy: 0.9843\n",
      "Epoch 142/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1202 - accuracy: 0.9871\n",
      "Epoch 143/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1196 - accuracy: 0.9871\n",
      "Epoch 144/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1191 - accuracy: 0.9871\n",
      "Epoch 145/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1186 - accuracy: 0.9857\n",
      "Epoch 146/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1180 - accuracy: 0.9857\n",
      "Epoch 147/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1175 - accuracy: 0.9871\n",
      "Epoch 148/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1171 - accuracy: 0.9871\n",
      "Epoch 149/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1166 - accuracy: 0.9871\n",
      "Epoch 150/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.1160 - accuracy: 0.9871: 0s - loss: 0.1247 - accuracy: 0.\n",
      "Epoch 151/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1156 - accuracy: 0.9886\n",
      "Epoch 152/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1151 - accuracy: 0.9886\n",
      "Epoch 153/200\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1146 - accuracy: 0.98 - 0s 5ms/step - loss: 0.1146 - accuracy: 0.9886\n",
      "Epoch 154/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1141 - accuracy: 0.9886\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1137 - accuracy: 0.9886\n",
      "Epoch 156/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.1132 - accuracy: 0.9886\n",
      "Epoch 157/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1127 - accuracy: 0.9886\n",
      "Epoch 158/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.1122 - accuracy: 0.9886\n",
      "Epoch 159/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1118 - accuracy: 0.9886\n",
      "Epoch 160/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1112 - accuracy: 0.9886\n",
      "Epoch 161/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1109 - accuracy: 0.9886\n",
      "Epoch 162/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1104 - accuracy: 0.9886\n",
      "Epoch 163/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1100 - accuracy: 0.9886\n",
      "Epoch 164/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1095 - accuracy: 0.9886\n",
      "Epoch 165/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1091 - accuracy: 0.9900\n",
      "Epoch 166/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.1086 - accuracy: 0.9900\n",
      "Epoch 167/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1082 - accuracy: 0.9900\n",
      "Epoch 168/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.1077 - accuracy: 0.9900\n",
      "Epoch 169/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1073 - accuracy: 0.9900\n",
      "Epoch 170/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1070 - accuracy: 0.9900\n",
      "Epoch 171/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1066 - accuracy: 0.9900\n",
      "Epoch 172/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1061 - accuracy: 0.9900\n",
      "Epoch 173/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1057 - accuracy: 0.9900\n",
      "Epoch 174/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.1053 - accuracy: 0.9900\n",
      "Epoch 175/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1049 - accuracy: 0.9900\n",
      "Epoch 176/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1045 - accuracy: 0.9900\n",
      "Epoch 177/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1041 - accuracy: 0.9900\n",
      "Epoch 178/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.1037 - accuracy: 0.9900\n",
      "Epoch 179/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1033 - accuracy: 0.9900\n",
      "Epoch 180/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1029 - accuracy: 0.9900\n",
      "Epoch 181/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1025 - accuracy: 0.9900\n",
      "Epoch 182/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1022 - accuracy: 0.9900\n",
      "Epoch 183/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1018 - accuracy: 0.9900\n",
      "Epoch 184/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1014 - accuracy: 0.9900\n",
      "Epoch 185/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1010 - accuracy: 0.9900\n",
      "Epoch 186/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1006 - accuracy: 0.9900\n",
      "Epoch 187/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1003 - accuracy: 0.9900\n",
      "Epoch 188/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0999 - accuracy: 0.9900\n",
      "Epoch 189/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0995 - accuracy: 0.9900: 0s - loss: 0.0884 - accuracy: 0.99\n",
      "Epoch 190/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0992 - accuracy: 0.9900\n",
      "Epoch 191/200\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0988 - accuracy: 0.9900\n",
      "Epoch 192/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0986 - accuracy: 0.9900\n",
      "Epoch 193/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0981 - accuracy: 0.9900\n",
      "Epoch 194/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0977 - accuracy: 0.9900\n",
      "Epoch 195/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0975 - accuracy: 0.9900\n",
      "Epoch 196/200\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0971 - accuracy: 0.9900\n",
      "Epoch 197/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0967 - accuracy: 0.9900\n",
      "Epoch 198/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0964 - accuracy: 0.9900\n",
      "Epoch 199/200\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0961 - accuracy: 0.9900\n",
      "Epoch 200/200\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0957 - accuracy: 0.9900\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.86      0.85        42\n",
      "        True       0.93      0.92      0.93        89\n",
      "\n",
      "    accuracy                           0.90       131\n",
      "   macro avg       0.88      0.89      0.89       131\n",
      "weighted avg       0.90      0.90      0.90       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid', input_shape=(1874, )))\n",
    "model.summary()\n",
    "model.compile(optimizer='Sgd', loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "epochs_hist = model.fit(train_X1, train_y1, epochs = 200) \n",
    "predict = model.predict(test_x1)\n",
    "y_predict = (predict > 0.5)\n",
    "print(classification_report(y_predict, test_y1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a66a4a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD  1\n",
      "==========completed\n",
      "FOLD  2\n",
      "==========completed\n",
      "FOLD  3\n",
      "==========completed\n",
      "FOLD  4\n",
      "==========completed\n",
      "FOLD  5\n",
      "==========completed\n",
      "          P1,   P2, All, P1&P2\n",
      "LR:      [0.85 0.83 0.86 0.87]\n",
      "RF-50:   [0.82 0.82 0.87 0.86]\n",
      "RF-100:  [0.82 0.84 0.87 0.87]\n",
      "RF-200:  [0.82 0.84 0.88 0.87]\n",
      "RF-400:  [0.82 0.85 0.88 0.87]\n"
     ]
    }
   ],
   "source": [
    "# For each method, these will hold 4 accuracy results for each of 5 folds\n",
    "# (1) accuracy of model trained on data from P1\n",
    "# (2) accuracy of model trained on data from P2\n",
    "# (3) accuracy of model trained on data from P1 and from P2\n",
    "# (4) accuracy of aggregation of model (1) and (2) from above\n",
    "LRresults = np.zeros((5, 4))\n",
    "RF50results = np.zeros((5, 4))\n",
    "RF100results = np.zeros((5, 4))\n",
    "RF200results = np.zeros((5, 4))\n",
    "RF400results = np.zeros((5, 4))\n",
    "\n",
    "kf1 = KFold(n_splits=5,shuffle = True,random_state = 42)\n",
    "kf2 = KFold(n_splits=5,shuffle = True,random_state = 42)\n",
    "\n",
    "\n",
    "i = 0\n",
    "for result1,result2 in zip(kf1.split(X1,y1),kf2.split(X2,y2)):\n",
    "  print(\"FOLD \", i+1)\n",
    "  X1_train, X1_test = X1.iloc[result1[0]], X1.iloc[result1[1]]\n",
    "  y1_train, y1_test = y1[result1[0]], y1[result1[1]]\n",
    "  X2_train, X2_test = X2.iloc[result2[0]], X2.iloc[result2[1]]\n",
    "  y2_train, y2_test = y2[result2[0]], y2[result2[1]]\n",
    "\n",
    "  X_train = X1_train.append(X2_train)\n",
    "  y_train = np.append(y1_train,y2_train)\n",
    "  X_test = X1_test.append(X2_test)\n",
    "  y_test = np.append(y1_test,y2_test)\n",
    "\n",
    "\n",
    "  ########## Train and test logistic regression models #################\n",
    "\n",
    "  clf1 = LogisticRegression(solver='liblinear',random_state=10000)\n",
    "  clf2 = LogisticRegression(solver='liblinear',random_state=10000)\n",
    "  clf = LogisticRegression(solver='liblinear',random_state=10000)\n",
    "  \n",
    "  clf1.fit(X1_train, y1_train)\n",
    "  accP1  = accuracy_score(y_test,clf1.predict(X_test))\n",
    "  \n",
    "  clf2.fit(X2_train, y2_train)\n",
    "  accP2 = accuracy_score(y_test,clf2.predict(X_test))\n",
    "    \n",
    "  clf.fit(X_train, y_train)\n",
    "  accALL = accuracy_score(y_test,clf.predict(X_test))\n",
    "  \n",
    "  # Merging of LR models\n",
    "  clf1.coef_ = (clf1.coef_ + clf2.coef_)/2\n",
    "  clf1.intercept_ = (clf1.intercept_ + clf2.intercept_)/2\n",
    "  accMERG = accuracy_score(y_test,clf1.predict(X_test))  \n",
    "\n",
    "  LRresults[i] = [accP1,accP2,accALL,accMERG]\n",
    "    \n",
    "\n",
    "  ########## Train and test RF models #################################\n",
    "\n",
    "  ntrees=50\n",
    "  RFresults=RF50results\n",
    "  evaluateForest(ntrees, RFresults, X_train, y_train, X1_train, y1_train, \n",
    "                         X2_train, y2_train, X_test, y_test) \n",
    "  \n",
    "  ntrees=100\n",
    "  RFresults=RF100results\n",
    "  evaluateForest(ntrees, RFresults, X_train, y_train, X1_train, y1_train, \n",
    "                         X2_train, y2_train, X_test, y_test) \n",
    "\n",
    "  ntrees=200\n",
    "  RFresults=RF200results\n",
    "  evaluateForest(ntrees, RFresults, X_train, y_train, X1_train, y1_train, \n",
    "                         X2_train, y2_train, X_test, y_test) \n",
    "\n",
    "  ntrees=400\n",
    "  RFresults=RF400results\n",
    "  evaluateForest(ntrees, RFresults, X_train, y_train, X1_train, y1_train, \n",
    "                         X2_train, y2_train, X_test, y_test) \n",
    "\n",
    "\n",
    "  print(\"==========completed\")\n",
    "  i = i + 1\n",
    "\n",
    "# Printing the averages over the 5 folds\n",
    "print(\"          P1,   P2, All, P1&P2\")\n",
    "np.set_printoptions(precision=2)\n",
    "print(\"LR:     \",np.mean(LRresults, axis=0))\n",
    "print(\"RF-50:  \",np.mean(RF50results, axis=0))\n",
    "print(\"RF-100: \",np.mean(RF100results, axis=0))\n",
    "print(\"RF-200: \",np.mean(RF200results, axis=0))\n",
    "print(\"RF-400: \",np.mean(RF400results, axis=0)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
